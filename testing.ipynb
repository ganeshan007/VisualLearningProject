{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "from predictor import ConvLayer, ReflectionPadConv, ResidualBlock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 16, 16])\n",
      "torch.Size([64, 3, 16, 16])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Tensor-likes are not close!\n\nMismatched elements: 49152 / 49152 (100.0%)\nGreatest absolute difference: 0.05875449627637863 at index (0, 2, 0, 0) (up to 1e-05 allowed)\nGreatest relative difference: 4.754302501678467 at index (0, 2, 0, 0) (up to 1e-05 allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(output1\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(output2\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 10\u001b[0m torch\u001b[39m.\u001b[39;49mtesting\u001b[39m.\u001b[39;49massert_close(output1, output2, rtol\u001b[39m=\u001b[39;49m\u001b[39m1e-5\u001b[39;49m, atol\u001b[39m=\u001b[39;49m\u001b[39m1e-5\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/torch/testing/_comparison.py:1511\u001b[0m, in \u001b[0;36massert_close\u001b[0;34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[0m\n\u001b[1;32m   1489\u001b[0m error_metas \u001b[39m=\u001b[39m not_close_error_metas(\n\u001b[1;32m   1490\u001b[0m     actual,\n\u001b[1;32m   1491\u001b[0m     expected,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1506\u001b[0m     msg\u001b[39m=\u001b[39mmsg,\n\u001b[1;32m   1507\u001b[0m )\n\u001b[1;32m   1509\u001b[0m \u001b[39mif\u001b[39;00m error_metas:\n\u001b[1;32m   1510\u001b[0m     \u001b[39m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mraise\u001b[39;00m error_metas[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto_error(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tensor-likes are not close!\n\nMismatched elements: 49152 / 49152 (100.0%)\nGreatest absolute difference: 0.05875449627637863 at index (0, 2, 0, 0) (up to 1e-05 allowed)\nGreatest relative difference: 4.754302501678467 at index (0, 2, 0, 0) (up to 1e-05 allowed)"
     ]
    }
   ],
   "source": [
    "input = torch.randn((64,3,32,32))\n",
    "c1 = ConvLayer(3,3,5,2)\n",
    "c2 = ReflectionPadConv(3,3,5,2)\n",
    "c1 = c1.eval()\n",
    "c2 = c2.eval()\n",
    "output1 = c1(input)\n",
    "output2 = c2(input)\n",
    "print(output1.shape)\n",
    "print(output2.shape)\n",
    "torch.testing.assert_close(output1, output2, rtol=1e-5, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2608.5713, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(torch.abs(output1-output2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    reflection_pad = self.reflection_pad(x);  x = None\n",
      "    conv2d = self.conv2d(reflection_pad);  reflection_pad = None\n",
      "    return conv2d\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from torch.fx import symbolic_trace\n",
    "\n",
    "symbolic_traced = symbolic_trace(c1)\n",
    "\n",
    "print(symbolic_traced.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualBlock(\n",
      "  (in1): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "  (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  (conv1): Module(\n",
      "    (padding): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (in2): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "  (conv2): Module(\n",
      "    (padding): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    in1 = self.in1(x)\n",
      "    relu = self.relu(in1);  in1 = None\n",
      "    conv1_padding = self.conv1.padding(relu);  relu = None\n",
      "    conv1_conv = self.conv1.conv(conv1_padding);  conv1_padding = None\n",
      "    in2 = self.in2(conv1_conv);  conv1_conv = None\n",
      "    relu_1 = self.relu(in2);  in2 = None\n",
      "    conv2_padding = self.conv2.padding(relu_1);  relu_1 = None\n",
      "    conv2_conv = self.conv2.conv(conv2_padding);  conv2_padding = None\n",
      "    add = conv2_conv + x;  conv2_conv = x = None\n",
      "    return add\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    }
   ],
   "source": [
    "from torch.fx import symbolic_trace\n",
    "res_block = ResidualBlock(3)\n",
    "print(symbolic_trace(res_block))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision 0.793722108310216 0.4393184535671582\n",
      "recall 0.8238810641627543 0.3919111111111111\n",
      "f1 0.8085204410725804 0.4142628958000564\n"
     ]
    }
   ],
   "source": [
    "tp = 4409\n",
    "fp = 5627\n",
    "fn = 6841\n",
    "tn = 26323\n",
    "precision_0 = tn / (tn+fn)\n",
    "recall_0 = tn / (tn+fp)\n",
    "precision_1 = tp / (tp+fp)\n",
    "recall_1 = tp / (tp+fn)\n",
    "f1_0 = 2*(recall_0*precision_0)/(recall_0+precision_0)\n",
    "f1_1 = 2*(recall_1*precision_1)/(recall_1+precision_1)\n",
    "\n",
    "print('precision',precision_0, precision_1)\n",
    "print('recall',recall_0, recall_1)\n",
    "print('f1',f1_0, f1_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((1,2,3,4))\n",
    "z = torch.randn((2,2))\n",
    "z = z.view(*z.shape,1,1).expand(*z.shape,x.shape[2],x.shape[3])\n",
    "# z = z.view(z.size(0), z.size(1), 1, 1).expand(z.shape[0], z.shape[1], x.shape[0], x.shape[1])\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "    # Get the dimensions of the input tensor\n",
    "    batch_size, num_features, height, width = input_tensor.size()\n",
    "\n",
    "    # Reshape the input tensor to a 2D matrix of shape (num_features, height * width)\n",
    "    features = input_tensor.view(batch_size * num_features, height * width)\n",
    "\n",
    "    # Calculate the Gram matrix by multiplying the reshaped features matrix with its transpose\n",
    "    gram = torch.matmul(features, features.t())\n",
    "\n",
    "    # Normalize the Gram matrix by dividing by the number of elements in each feature map\n",
    "    gram /= (batch_size * num_features * height * width)\n",
    "\n",
    "    return gram\n",
    "def gram2(input):\n",
    "    a, b, c, d = input.size()  # a=batch size(=1)\n",
    "    # b=number of feature maps\n",
    "    # (c,d)=dimensions of a f. map (N=c*d)\n",
    "\n",
    "    features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n",
    "\n",
    "    G = torch.mm(features, features.t())  # compute the gram product\n",
    "    # we 'normalize' the values of the gram matrix\n",
    "    # by dividing by the number of element in each feature maps.\n",
    "    return G.div(a * b * c * d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "features = torch.randn((1,2,3,4))\n",
    "f1 = gram_matrix(features)\n",
    "f2 = gram2(features)\n",
    "torch.testing.assert_close(f1, f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.fx import symbolic_trace\n",
    "from allsome import *\n",
    "nl_layer = get_non_linearity(layer_type='lrelu')\n",
    "norm_layer = get_norm_layer(layer_type='batch')\n",
    "netE = E_ResNet(input_nc=3, output_nc=1, ndf=64, n_blocks=4, norm_layer=norm_layer,\n",
    "                        nl_layer=nl_layer, gpu_ids=[], vaeLike=False)\n",
    "sample_input = torch.randn(64, 3, 128, 128)\n",
    "output = netE(sample_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/i1nIvaejHZg/i1nIvaejHZg_1 2311\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/AF9A0Raa63M/AF9A0Raa63M_1 91\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/AF9A0Raa63M/AF9A0Raa63M_2 76\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/av0WFwFMB8g/av0WFwFMB8g_1 796\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/0_eqVxGOjuI/0_eqVxGOjuI_1 511\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/2jI_2QvtEQE/2jI_2QvtEQE_1 222\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/3aegvVRcBqI/3aegvVRcBqI_1 177\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/fsa7x8JsFx4/fsa7x8JsFx4_1 961\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/8KlRCq5SDTE/8KlRCq5SDTE_1 13\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/8KlRCq5SDTE/8KlRCq5SDTE_2 21\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Iq9TrDaLpMM/Iq9TrDaLpMM_1 156\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/dqyhMFgZhVc/dqyhMFgZhVc_1 1321\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/dqyhMFgZhVc/dqyhMFgZhVc_4 14\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/dqyhMFgZhVc/dqyhMFgZhVc_3 29\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/dqyhMFgZhVc/dqyhMFgZhVc_2 991\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/dqyhMFgZhVc/dqyhMFgZhVc_5 179\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/67Pv6S5KAnM/67Pv6S5KAnM_1 259\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/07U1fSrk9oI/07U1fSrk9oI_1 4756\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Ei_PONHmJnA/Ei_PONHmJnA_1 1915\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/bI_ctI4Ww5g/bI_ctI4Ww5g_1 158\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/ij8kv1Iltr4/ij8kv1Iltr4_1 661\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Dpc07Eipo80/Dpc07Eipo80_1 586\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/l7P2R3j9NkM/l7P2R3j9NkM_1 14\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/i7mlhv80A_k/i7mlhv80A_k_1 133\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/i7mlhv80A_k/i7mlhv80A_k_3 13\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/i7mlhv80A_k/i7mlhv80A_k_4 55\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/i7mlhv80A_k/i7mlhv80A_k_5 308\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/i7mlhv80A_k/i7mlhv80A_k_2 196\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/5_e4iaW7a54/5_e4iaW7a54_2 796\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/5_e4iaW7a54/5_e4iaW7a54_1 106\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/bUx62eZDfW0/bUx62eZDfW0_1 1096\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/bUx62eZDfW0/bUx62eZDfW0_2 1321\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/5WVkPmPTrv8/5WVkPmPTrv8_1 1613\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/KDzOg-4_Ix8/KDzOg-4_Ix8_4 13\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/KDzOg-4_Ix8/KDzOg-4_Ix8_3 50\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/KDzOg-4_Ix8/KDzOg-4_Ix8_2 16\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/KDzOg-4_Ix8/KDzOg-4_Ix8_1 4\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/KwlT0OFRQys/KwlT0OFRQys_1 1276\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/6B0Kd9vk11Y/6B0Kd9vk11Y_2 50\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/6B0Kd9vk11Y/6B0Kd9vk11Y_1 31\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/DgXUijYb8fE/DgXUijYb8fE_1 436\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/DgXUijYb8fE/DgXUijYb8fE_3 466\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/DgXUijYb8fE/DgXUijYb8fE_2 886\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/7pMAUHPRYrc/7pMAUHPRYrc_1 2098\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/7pMAUHPRYrc/7pMAUHPRYrc_2 106\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/4L1ggPJFDNI/4L1ggPJFDNI_1 691\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/iEiMdRqzP9I/iEiMdRqzP9I_2 196\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/iEiMdRqzP9I/iEiMdRqzP9I_5 181\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/iEiMdRqzP9I/iEiMdRqzP9I_4 76\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/iEiMdRqzP9I/iEiMdRqzP9I_3 46\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/iEiMdRqzP9I/iEiMdRqzP9I_1 31\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/df4BvbAvXi8/df4BvbAvXi8_1 2856\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/CZxqkUoEFDM/CZxqkUoEFDM_1 1019\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/BgKtyBFsiD0/BgKtyBFsiD0_1 145\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/jZOLRAIUW2s/jZOLRAIUW2s_3 301\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/jZOLRAIUW2s/jZOLRAIUW2s_4 316\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/jZOLRAIUW2s/jZOLRAIUW2s_2 104\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/jZOLRAIUW2s/jZOLRAIUW2s_1 151\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/0Rn5D2-kMTY/0Rn5D2-kMTY_2 361\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/0Rn5D2-kMTY/0Rn5D2-kMTY_1 271\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/dtD6co0f-eU/dtD6co0f-eU_1 2590\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/FofT2rdLT1E/FofT2rdLT1E_1 781\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/IlMFL8RxgeY/IlMFL8RxgeY_4 31\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/IlMFL8RxgeY/IlMFL8RxgeY_2 396\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/IlMFL8RxgeY/IlMFL8RxgeY_5 36\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/IlMFL8RxgeY/IlMFL8RxgeY_9 31\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/IlMFL8RxgeY/IlMFL8RxgeY_7 0\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/IlMFL8RxgeY/IlMFL8RxgeY_6 3\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/IlMFL8RxgeY/IlMFL8RxgeY_1 4\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/IlMFL8RxgeY/IlMFL8RxgeY_8 6\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/3-VAM9owl-o/3-VAM9owl-o_1 646\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/9Cj0pMGQKPI/9Cj0pMGQKPI_1 4223\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/1G4AxSn3meA/1G4AxSn3meA_1 134\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/1G4AxSn3meA/1G4AxSn3meA_2 166\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/jZhkQrNmsHM/jZhkQrNmsHM_1 16\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/LCroFHF5xwI/LCroFHF5xwI_1 369\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/AI1N7oelEW0/AI1N7oelEW0_1 89\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Dcut4Fb_BE0/Dcut4Fb_BE0_2 256\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Dcut4Fb_BE0/Dcut4Fb_BE0_1 582\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/hnhHnm_paEk/hnhHnm_paEk_1 211\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/invEol0uQdE/invEol0uQdE_1 3811\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/5ap1G00n2zw/5ap1G00n2zw_1 3\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/6h2cEt68qwM/6h2cEt68qwM_1 802\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/HxCwbCD3Kwk/HxCwbCD3Kwk_3 19\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/HxCwbCD3Kwk/HxCwbCD3Kwk_2 10\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/HxCwbCD3Kwk/HxCwbCD3Kwk_1 33\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/41kZ-0cFNLk/41kZ-0cFNLk_1 115\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/DTaROxnWsZc/DTaROxnWsZc_1 182\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/DTaROxnWsZc/DTaROxnWsZc_2 10\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/DTaROxnWsZc/DTaROxnWsZc_3 256\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/45ZUiuXHiRg/45ZUiuXHiRg_1 2431\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Hrapw1xcM0k/Hrapw1xcM0k_14 87\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Hrapw1xcM0k/Hrapw1xcM0k_13 136\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Hrapw1xcM0k/Hrapw1xcM0k_12 376\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Hrapw1xcM0k/Hrapw1xcM0k_15 61\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Hrapw1xcM0k/Hrapw1xcM0k_4 226\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Hrapw1xcM0k/Hrapw1xcM0k_3 181\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Hrapw1xcM0k/Hrapw1xcM0k_5 91\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Hrapw1xcM0k/Hrapw1xcM0k_21 211\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Hrapw1xcM0k/Hrapw1xcM0k_19 241\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Hrapw1xcM0k/Hrapw1xcM0k_10 136\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Hrapw1xcM0k/Hrapw1xcM0k_17 44\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Hrapw1xcM0k/Hrapw1xcM0k_16 286\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Hrapw1xcM0k/Hrapw1xcM0k_11 61\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Hrapw1xcM0k/Hrapw1xcM0k_18 76\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Hrapw1xcM0k/Hrapw1xcM0k_20 5\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Hrapw1xcM0k/Hrapw1xcM0k_9 316\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Hrapw1xcM0k/Hrapw1xcM0k_7 436\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Hrapw1xcM0k/Hrapw1xcM0k_6 46\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Hrapw1xcM0k/Hrapw1xcM0k_1 136\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/Hrapw1xcM0k/Hrapw1xcM0k_8 136\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/a7W4jTkogfA/a7W4jTkogfA_1 20\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/J3gdaWt8hpI/J3gdaWt8hpI_1 26\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/6kO4bsG0w4U/6kO4bsG0w4U_1 271\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/IAA95bBo2z0/IAA95bBo2z0_1 241\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/km-v_w5Wr24/km-v_w5Wr24_1 106\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/8jkA3S5vRVc/8jkA3S5vRVc_1 361\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/5C9ehMEUZdI/5C9ehMEUZdI_2 10\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/5C9ehMEUZdI/5C9ehMEUZdI_1 210\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/8TqIzuWVGxc/8TqIzuWVGxc_2 7\n",
      "/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test/8TqIzuWVGxc/8TqIzuWVGxc_1 12\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "dir = '/Users/ganeshanmalhotra/Desktop/Quarter3/Visual/VisualLearningProject/sky_timelapse/sky_test'\n",
    "dir_list = glob.glob(dir + '/*' + '/*')\n",
    "for dir in dir_list:\n",
    "    fnames = glob.glob(dir+\"/*.jpg\")\n",
    "    print(dir, len(fnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 64, 3, 3], expected input[1, 128, 64, 64] to have 64 channels, but got 128 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential( nn\u001b[39m.\u001b[39mLeakyReLU(\u001b[39m0.2\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), \n\u001b[1;32m      2\u001b[0m                        nn\u001b[39m.\u001b[39mConv2d(\u001b[39m64\u001b[39m,\u001b[39m128\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m      3\u001b[0m                        nn\u001b[39m.\u001b[39mLeakyReLU(\u001b[39m0.2\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m      4\u001b[0m                        nn\u001b[39m.\u001b[39mConv2d(\u001b[39m64\u001b[39m, \u001b[39m128\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m      5\u001b[0m                        nn\u001b[39m.\u001b[39mAvgPool2d(kernel_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[1;32m      7\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m y \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(y\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 64, 3, 3], expected input[1, 128, 64, 64] to have 64 channels, but got 128 channels instead"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential( nn.LeakyReLU(0.2, inplace=True), \n",
    "                       nn.Conv2d(64,128, kernel_size=3, stride=1, padding=1),\n",
    "                       nn.LeakyReLU(0.2, inplace=True),\n",
    "                       nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "                       nn.AvgPool2d(kernel_size=2, stride=2, padding=0))\n",
    "\n",
    "x = torch.randn(1, 64, 64, 64)\n",
    "y = model(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
